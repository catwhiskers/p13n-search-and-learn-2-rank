{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCeYA79m1DEX"
   },
   "source": [
    "# Recommending movies: ranking\n",
    "\n",
    "reference to https://www.tensorflow.org/recommenders/examples/quickstart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gf2jMHkZQYB5"
   },
   "source": [
    "## Imports\n",
    "\n",
    "\n",
    "Let's first get our imports out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-01T11:09:17.138318Z",
     "iopub.status.busy": "2020-10-01T11:09:17.137597Z",
     "iopub.status.idle": "2020-10-01T11:09:20.159728Z",
     "shell.execute_reply": "2020-10-01T11:09:20.159147Z"
    },
    "id": "9gG3jLOGbaUv"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade tensorflow==2.3.0\n",
    "!pip install -q --upgrade tensorflow-datasets\n",
    "!pip install -q --upgrade tensorflow-recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-01T11:09:20.165023Z",
     "iopub.status.busy": "2020-10-01T11:09:20.164320Z",
     "iopub.status.idle": "2020-10-01T11:09:26.874394Z",
     "shell.execute_reply": "2020-10-01T11:09:26.873770Z"
    },
    "id": "SZGYDaF-m5wZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-01T11:09:26.878846Z",
     "iopub.status.busy": "2020-10-01T11:09:26.878142Z",
     "iopub.status.idle": "2020-10-01T11:09:26.886254Z",
     "shell.execute_reply": "2020-10-01T11:09:26.886686Z"
    },
    "id": "BxQ_hy7xPH3N"
   },
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PAqjR4a1RR4"
   },
   "source": [
    "## Preparing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-01T11:09:26.892939Z",
     "iopub.status.busy": "2020-10-01T11:09:26.892211Z",
     "iopub.status.idle": "2020-10-01T11:10:35.383186Z",
     "shell.execute_reply": "2020-10-01T11:10:35.382691Z"
    },
    "id": "aaQhqcLGP0jL"
   },
   "outputs": [],
   "source": [
    "ratings = tfds.load(\"movie_lens/100k-ratings\", split=\"train\")\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_id\": x[\"movie_id\"],\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"user_rating\"],\n",
    "    \"timestamp\": x[\"timestamp\"]\n",
    "})\n",
    "\n",
    "\n",
    "for x in ratings.take(10).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie ID Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "movie_id_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_lookup.adapt(ratings.map(lambda x: x[\"movie_id\"]))\n",
    "\n",
    "print(f\"Vocabulary: {movie_id_lookup.get_vocabulary()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_lookup([\"50\", \"258\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the hashing approach.\n",
    "    input_dim=movie_id_lookup.vocab_size(),\n",
    "    output_dim=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_model = tf.keras.Sequential([movie_id_lookup, movie_id_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_model([\"258\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User ID Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hashing_bins = 20_000\n",
    "user_id_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
    "user_id_embedding = tf.keras.layers.Embedding(num_hashing_bins, 32)\n",
    "\n",
    "user_id_lookup.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
    "\n",
    "user_id_model = tf.keras.Sequential([user_id_lookup, user_id_embedding])\n",
    "user_id_model([\"1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iu4XSa_G1nyN"
   },
   "source": [
    "We'll split the data by putting 80% of the ratings in the train set, and 20% in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-01T11:10:35.388468Z",
     "iopub.status.busy": "2020-10-01T11:10:35.387786Z",
     "iopub.status.idle": "2020-10-01T11:10:35.391929Z",
     "shell.execute_reply": "2020-10-01T11:10:35.391452Z"
    },
    "id": "rS0eDfkjnjJL"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "    print(f\"Timestamp: {x['timestamp']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    tf.cast(0, tf.int64), tf.maximum).numpy().max()\n",
    "min_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    np.int64(1e9), tf.minimum).numpy().min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000)\n",
    "\n",
    "print(f\"Buckets: {timestamp_buckets[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_embedding_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
    "  tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32)\n",
    "])\n",
    "\n",
    "for timestamp in ratings.take(1).map(lambda x: x[\"timestamp\"]).batch(1).as_numpy_iterator():\n",
    "    print(f\"Timestamp embedding: {timestamp_embedding_model(timestamp)}.\")                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text = tf.keras.layers.experimental.preprocessing.TextVectorization()\n",
    "title_text.adapt(ratings.map(lambda x: x[\"movie_title\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in ratings.batch(1).map(lambda x: x[\"movie_title\"]).take(1):\n",
    "    print(title_text(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text.get_vocabulary()[40:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        num_hashing_bins = 20_000\n",
    "\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            user_id_lookup,\n",
    "            tf.keras.layers.Embedding(user_id_lookup.vocab_size(), 32),\n",
    "        ])\n",
    "        self.timestamp_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
    "            tf.keras.layers.Embedding(len(timestamp_buckets) + 2, 32)\n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "      # Take the input dictionary, pass it through each input layer,\n",
    "      # and concatenate the result.\n",
    "\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"user_id\"]),\n",
    "            self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        max_tokens = 10_000\n",
    "\n",
    "        self.id_embedding = tf.keras.Sequential([\n",
    "            movie_id_lookup,\n",
    "            tf.keras.layers.Embedding(movie_id_lookup.vocab_size(), 32)\n",
    "        ])\n",
    "        self.title_text_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=max_tokens),\n",
    "            tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "          # We average the embedding of individual words to get one embedding vector\n",
    "          # per title.\n",
    "            tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.concat([\n",
    "            self.id_embedding(inputs[\"movie_id\"]),\n",
    "            self.title_text_embedding(inputs[\"movie_title\"]),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-01T11:10:35.397436Z",
     "iopub.status.busy": "2020-10-01T11:10:35.396767Z",
     "iopub.status.idle": "2020-10-01T11:10:43.808904Z",
     "shell.execute_reply": "2020-10-01T11:10:43.808238Z"
    },
    "id": "MKROCiPo_5LJ"
   },
   "outputs": [],
   "source": [
    "for row in ratings.batch(10).map(lambda x: x[\"movie_title\"]).take(1):\n",
    "  print(title_text(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_model = MovieModel()\n",
    "\n",
    "movie_model.title_text_embedding.layers[0].adapt(\n",
    "    ratings.map(lambda x: x[\"movie_title\"]))\n",
    "\n",
    "for row in ratings.batch(1).take(1):\n",
    "    print(f\"Computed representations: {movie_model(row)[0, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-Vj9nHb48pn"
   },
   "source": [
    "## Implementing a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCi-seR86qqa"
   },
   "source": [
    "### Architecture\n",
    "\n",
    "Ranking models do not face the same efficiency constrains as retrieval models do, and so we have a little bit more freedom in our choice of architectures.\n",
    "\n",
    "A model composed of multiple stacked dense layers is a relatively common architecture for ranking tasks. We can implement it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-01T11:10:43.817997Z",
     "iopub.status.busy": "2020-10-01T11:10:43.817260Z",
     "iopub.status.idle": "2020-10-01T11:10:43.819649Z",
     "shell.execute_reply": "2020-10-01T11:10:43.819101Z"
    },
    "id": "fAk0y0Yf1eGh"
   },
   "outputs": [],
   "source": [
    "class RankingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        # Compute embeddings for users.\n",
    "        self.user_embeddings = UserModel()\n",
    "\n",
    "        # Compute embeddings for movies.\n",
    "        self.movie_embeddings = MovieModel()\n",
    "\n",
    "        # Compute predictions.\n",
    "        self.ratings = tf.keras.Sequential([\n",
    "          # Learn multiple dense layers.\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "          # Make rating predictions in the final layer.\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_embedding = self.user_embeddings(inputs)\n",
    "        movie_embedding = self.movie_embeddings(inputs)\n",
    "        return self.ratings(tf.concat([user_embedding, movie_embedding], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g76wZt-s2WmS"
   },
   "source": [
    "This model takes user ids and movie titles, and outputs a predicted rating:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCaCqJsXSkCo"
   },
   "source": [
    "### Loss and metrics\n",
    "\n",
    "The next component is the loss used to train our model. TFRS has several loss layers and tasks to make this easy.\n",
    "\n",
    "In this instance, we'll make use of the `Ranking` task object: a convenience wrapper that bundles together the loss function and metric computation. \n",
    "\n",
    "We'll use it together with the `MeanSquaredError` Keras loss in order to predict the ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "task = tfrs.tasks.Ranking(\n",
    "  loss = tf.keras.losses.MeanSquaredError(),\n",
    "  metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9-3xFC-1cbz0"
   },
   "source": [
    "The task itself is a Keras layer that takes true and predicted as arguments, and returns the computed loss. We'll use that to implement the model's training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZUFeSlWRHGx"
   },
   "source": [
    "### The full model\n",
    "\n",
    "We can now put it all together into a model. TFRS exposes a base model class (`tfrs.models.Model`) which streamlines bulding models: all we need to do is to set up the components in the `__init__` method, and implement the `compute_loss` method, taking in the raw features and returning a loss value.\n",
    "\n",
    "The base model will then take care of creating the appropriate training loop to fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-01T11:10:44.026734Z",
     "iopub.status.busy": "2020-10-01T11:10:44.026038Z",
     "iopub.status.idle": "2020-10-01T11:10:44.028098Z",
     "shell.execute_reply": "2020-10-01T11:10:44.027626Z"
    },
    "id": "8n7c5CHFp0ow"
   },
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ranking_model: tf.keras.Model = RankingModel()\n",
    "        self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "            loss = tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        rating_predictions = self.ranking_model(\n",
    "            features)\n",
    "        print('rating_predictions', rating_predictions)\n",
    "        # The task computes the loss and the metrics.\n",
    "        return self.task(labels=features[\"user_rating\"], predictions=rating_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDN_LJGlnRGo"
   },
   "source": [
    "## Fitting and evaluating\n",
    "\n",
    "After defining the model, we can use standard Keras fitting and evaluation routines to fit and evaluate the model.\n",
    "\n",
    "Let's first instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-01T11:10:44.034353Z",
     "iopub.status.busy": "2020-10-01T11:10:44.033687Z",
     "iopub.status.idle": "2020-10-01T11:10:44.075740Z",
     "shell.execute_reply": "2020-10-01T11:10:44.075160Z"
    },
    "id": "aW63YaqP2wCf"
   },
   "outputs": [],
   "source": [
    "model = MovielensModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nma0vc2XdN5g"
   },
   "source": [
    "Then shuffle, batch, and cache the training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-01T11:10:44.080235Z",
     "iopub.status.busy": "2020-10-01T11:10:44.079593Z",
     "iopub.status.idle": "2020-10-01T11:10:44.083549Z",
     "shell.execute_reply": "2020-10-01T11:10:44.083037Z"
    },
    "id": "53QJwY1gUnfv"
   },
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(10_000).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8mHTxKAdTJO"
   },
   "source": [
    "Then train the  model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-01T11:10:44.087970Z",
     "iopub.status.busy": "2020-10-01T11:10:44.087309Z",
     "iopub.status.idle": "2020-10-01T11:10:45.480649Z",
     "shell.execute_reply": "2020-10-01T11:10:45.481122Z"
    },
    "id": "ZxPntlT8EFOZ"
   },
   "outputs": [],
   "source": [
    "model.fit(cached_train, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YsluR8audV9W"
   },
   "source": [
    "As the model trains, the loss is falling and the RMSE metric is improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Gxp5RLFcv64"
   },
   "source": [
    "Finally, we can evaluate our model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-10-01T11:10:45.486157Z",
     "iopub.status.busy": "2020-10-01T11:10:45.485473Z",
     "iopub.status.idle": "2020-10-01T11:10:46.048770Z",
     "shell.execute_reply": "2020-10-01T11:10:46.048225Z"
    },
    "id": "W-zu6HLODNeI"
   },
   "outputs": [],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "rerank_user = 88\n",
    "user_id = str(rerank_user)\n",
    "user_item_df = pd.read_pickle(\"user_item_df.p\")\n",
    "item_df = pd.read_pickle(\"item_df.p\")\n",
    "genres = ['unknown','Action' , 'Adventure', 'Animation', 'Childrens' , 'Comedy' , 'Crime', \\\n",
    "                                        'Documentary', 'Drama' ,'Fantasy' , 'Film-Noir' , 'Horror' , 'Musical', \\\n",
    "                                        'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "\n",
    "def plot_heat_map(df, figsize=(10,7)): \n",
    "    df = df.div(df.sum(axis=1), axis=0)     \n",
    "    plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(df)\n",
    "\n",
    "\n",
    "u_id = user_id\n",
    "\n",
    "tester_df = user_item_df[user_item_df['uid']==int(u_id)]\n",
    "tester_df['positive'] = tester_df['rating'] >3 \n",
    "\n",
    "review = tester_df[['positive']+genres].groupby(['positive']).sum()\n",
    "plot_heat_map(review, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_user=\"yianc\"\n",
    "master_user_password=\"Cj;6qo4fu60218\"\n",
    "elastic_search_endpoint=\"search-p13n-search-demo-gezwjq5aol2p7u2gvje4ni7oom.us-west-2.es.amazonaws.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "r = requests.get('https://{}/movies/movie/_search?q=Drama:1&size=100'.format(elastic_search_endpoint), auth=(master_user, master_user_password))\n",
    "rjson = r.json()\n",
    "rjson \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_from_search = [] \n",
    "for h in rjson['hits']['hits']:\n",
    "    items_from_search.append(h['_source']['iid'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def get_input_dic_by_movie_user_id(user_id, movie_id, movie_df=item_df):\n",
    "    \"\"\"\n",
    "    This takes in an artist_id from Personalize so it will be a string,\n",
    "    converts it to an int, and then does a lookup in a default or specified\n",
    "    dataframe.\n",
    "    \n",
    "    A really broad try/except clause was added in case anything goes wrong.\n",
    "    \n",
    "    Feel free to add more debugging or filtering here to improve results if\n",
    "    you hit an error.\n",
    "    \"\"\"\n",
    "\n",
    "    title_str = movie_df[movie_df['iid']==int(movie_id)].iloc[0]['title']\n",
    "    mid = tf.convert_to_tensor([str(movie_id)])\n",
    "    uid = tf.convert_to_tensor([str(user_id)])\n",
    "    title = tf.convert_to_tensor([movie_df[movie_df['iid']==int(movie_id)].iloc[0]['title']])\n",
    "    timestamp = tf.convert_to_tensor([time.time()])\n",
    "    res = {} \n",
    "    res['movie_id'] = mid\n",
    "    res['user_id'] = uid\n",
    "    res['movie_title'] = title \n",
    "    res['timestamp'] = timestamp\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_movie_by_id(movie_id, movie_df=item_df):\n",
    "    \"\"\"\n",
    "    This takes in an artist_id from Personalize so it will be a string,\n",
    "    converts it to an int, and then does a lookup in a default or specified\n",
    "    dataframe.\n",
    "    \n",
    "    A really broad try/except clause was added in case anything goes wrong.\n",
    "    \n",
    "    Feel free to add more debugging or filtering here to improve results if\n",
    "    you hit an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        c_row = movie_df[movie_df['iid']==movie_id].iloc[0]\n",
    "        title = c_row['title'] \n",
    "        m_genres = [] \n",
    "        for g in genres: \n",
    "            if c_row[g] == 1: \n",
    "                m_genres.append(g)\n",
    "        return movie_df[movie_df['iid']==movie_id].iloc[0]['title'] + \" genres:\" + \",\".join(m_genres)\n",
    "    except:\n",
    "        return \"Error obtaining movie info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_list = []\n",
    "for item in items_from_search:\n",
    "    movie = get_movie_by_id(item)\n",
    "    rerank_list.append(movie)\n",
    "rerank_df = pd.DataFrame(rerank_list, columns = [rerank_user])\n",
    "rerank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy \n",
    "ranked_list = []\n",
    "for item in items_from_search: \n",
    "    movie = get_movie_by_id(item)\n",
    "    inf_input = get_input_dic_by_movie_user_id(str(rerank_user),str(item))\n",
    "    score = model.ranking_model(inf_input)\n",
    "    ranked_list.append([movie, score.numpy()[0][0]])\n",
    "\n",
    "ranked_list = sorted(ranked_list, key=lambda x:x[1], reverse=True)   \n",
    "ranked_df = pd.DataFrame(numpy.array(ranked_list)[:,0], columns = ['Re-Ranked'])\n",
    "all_df = pd.concat([rerank_df, ranked_df], axis=1)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKZyP9A1dxit"
   },
   "source": [
    "The lower the RMSE metric, the more accurate our model is at predicting ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "basic_ranking.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
